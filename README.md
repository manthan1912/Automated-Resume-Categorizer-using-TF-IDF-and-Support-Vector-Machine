# Automated-Resume-Categorizer-using-TF-IDF-and-Support-Vector-Machine

Classify resumes into common job categories using a simple, fast, and reliable NLP pipeline:
**clean text â†’ TF-IDF â†’ LinearSVC**.  
Train the model on your CSV, then run a minimal **Streamlit** app to predict from uploaded resumes (PDF/TXT/DOCX).

---

## ğŸ”§ Whatâ€™s in this project

- `app1.py` â€” Minimal Streamlit app (upload a resume â†’ get 1 predicted category)  
  - Robust PDF/DOCX parsing (PyMuPDF â†’ PyPDF2 â†’ pdfminer â†’ text fallback)  
  - Same cleaner as training, guards against low-text/garbage inputs
- `requirements.txt` â€” All dependencies for training + app
- `artifacts/` â€” Model files created after training  
  - `tfidf.pkl` (fitted `TfidfVectorizer`)  
  - `clf.pkl` (fitted `LinearSVC`)
- `resume_classifier_notebook.ipynb` â€” Notebook to train/evaluate and export the two artifacts
- `data.csv` â€” Your training data (columns: **Category**, **Resume**)

---

## ğŸš€ Quickstart

```bash
# 1) Create env & install dependencies
python -m venv .venv
source .venv/bin/activate        # on Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 2) Train the model (open the notebook and run all cells)
# This creates: artifacts/tfidf.pkl and artifacts/clf.pkl

# 3) Run the app
streamlit run app1.py
```

Open the URL shown in the terminal, upload a resume (PDF/TXT/DOCX), and youâ€™ll see one predicted category.

---

## ğŸ“¦ Data format

`data.csv` must contain:

| Column    | Type   | Notes                         |
|-----------|--------|-------------------------------|
| Category  | string | e.g., `Data Science`, `HR`    |
| Resume    | string | raw resume text               |

The app uses a fixed **ID â†’ label** mapping during training and inference. If you change labels in your data, update both the notebook and `app1.py` mapping accordingly.

Current mapping:

```
0: Advocate
1: Arts
2: Automation Testing
3: Blockchain
4: Business Analyst
5: Civil Engineer
6: Data Science
7: Database
8: DevOps Engineer
9: DotNet Developer
10: ETL Developer
11: Electrical Engineering
12: HR
13: Hadoop
14: Health and fitness
15: Java Developer
16: Mechanical Engineer
17: Network Security Engineer
18: Operations Manager
19: PMO
20: Python Developer
21: SAP Developer
22: Sales
23: Testing
24: Web Designing
```

---

## ğŸ§ª Training & evaluation

Use `resume_classifier_notebook.ipynb`:

- Cleans text (regex-based), vectorizes with TF-IDF (1â€“2 grams, sublinear tf)
- Trains `LinearSVC` with stratified split
- Optional 5-fold CV (pipeline-based so it doesnâ€™t mutate your saved artifacts)
- Saves `artifacts/tfidf.pkl` and `artifacts/clf.pkl`

> Keep the **same cleaner** in both training and app for consistent results.

---

## ğŸ–¥ï¸ Running the app

```bash
streamlit run app1.py
```

- Upload **text-based** PDF, TXT, or DOCX.  
- The app blocks predictions if too little text is extracted (prevents garbage results).

---

## ğŸ§° Dependencies

See `requirements.txt`. Key ones:

- **ML:** `scikit-learn`, `numpy`, `pandas`
- **App:** `streamlit`
- **Parsing:** `pymupdf` (PyMuPDF), `PyPDF2`, `pdfminer.six`, `python-docx`
- **(Optional) EDA:** `matplotlib`, `seaborn`

Install with:

```bash
pip install -r requirements.txt
```

## ğŸ“ Suggested repo layout

```
.
â”œâ”€â”€ app1.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ resume_classifier_notebook.ipynb
â”œâ”€â”€ data.csv
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ tfidf.pkl
â”‚   â””â”€â”€ clf.pkl
â””â”€â”€ sample_resumes/        # optional test PDFs
    â”œâ”€â”€ Advocate_sample_resume.pdf
    â”œâ”€â”€ Civil_Engineer_sample_resume.pdf
    â”œâ”€â”€ SAP_Developer_sample_resume.pdf
    â””â”€â”€ Network_Security_Engineer_sample_resume.pdf
```

---
