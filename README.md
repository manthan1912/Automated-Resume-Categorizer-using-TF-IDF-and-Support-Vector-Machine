# Resume Screening ML App (TF-IDF + LinearSVC)

Classify resumes into common job categories using a simple, fast, and reliable NLP pipeline:
**clean text â†’ TF-IDF â†’ LinearSVC**.  
Train the model on your CSV, then run a minimal **Streamlit** app to predict from uploaded resumes (PDF/TXT/DOCX).

---

## ğŸ”§ Whatâ€™s in this project

- `app1.py` â€” Minimal Streamlit app (upload a resume â†’ get 1 predicted category)  
  - Robust PDF/DOCX parsing (PyMuPDF â†’ PyPDF2 â†’ pdfminer â†’ text fallback)  
  - Same cleaner as training, guards against low-text/garbage inputs
- `requirements.txt` â€” All dependencies for training + app
- `artifacts/` â€” Model files created after training  
  - `tfidf.pkl` (fitted `TfidfVectorizer`)  
  - `clf.pkl` (fitted `LinearSVC`)
- `resume_classifier_notebook.ipynb` â€” Notebook to train/evaluate and export the two artifacts
- `data.csv` â€” Your training data (columns: **Category**, **Resume**)

> You may also have `Untitled-1.py` from an earlier notebook export; prefer using the notebook above for clean training.

---

## ğŸš€ Quickstart

```bash
# 1) Create env & install deps
python -m venv .venv
source .venv/bin/activate        # on Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 2) Train the model (open the notebook and run all cells)
# This creates: artifacts/tfidf.pkl and artifacts/clf.pkl

# 3) Run the app
streamlit run app1.py
```

Open the URL shown in the terminal, upload a resume (PDF/TXT/DOCX), and youâ€™ll see one predicted category.

---

## ğŸ“¦ Data format

`data.csv` must contain:

| Column    | Type   | Notes                         |
|-----------|--------|-------------------------------|
| Category  | string | e.g., `Data Science`, `HR`    |
| Resume    | string | raw resume text               |

The app uses a fixed **ID â†’ label** mapping during training and inference. If you change labels in your data, update both the notebook and `app1.py` mapping accordingly.

Current mapping:

```
0: Advocate
1: Arts
2: Automation Testing
3: Blockchain
4: Business Analyst
5: Civil Engineer
6: Data Science
7: Database
8: DevOps Engineer
9: DotNet Developer
10: ETL Developer
11: Electrical Engineering
12: HR
13: Hadoop
14: Health and fitness
15: Java Developer
16: Mechanical Engineer
17: Network Security Engineer
18: Operations Manager
19: PMO
20: Python Developer
21: SAP Developer
22: Sales
23: Testing
24: Web Designing
```

---

## ğŸ§ª Training & evaluation

Use `resume_classifier_notebook.ipynb`:

- Cleans text (regex-based), vectorizes with TF-IDF (1â€“2 grams, sublinear tf)
- Trains `LinearSVC` with stratified split
- Optional 5-fold CV (pipeline-based so it doesnâ€™t mutate your saved artifacts)
- Saves `artifacts/tfidf.pkl` and `artifacts/clf.pkl`

> Keep the **same cleaner** in both training and app for consistent results.

---

## ğŸ–¥ï¸ Running the app

```bash
streamlit run app1.py
```

- Upload **text-based** PDF, TXT, or DOCX.  
- For scanned PDFs (images), run OCR first:
  ```
  ocrmypdf input.pdf output.pdf
  ```
- The app blocks predictions if too little text is extracted (prevents garbage results).

---

## ğŸ§° Dependencies

See `requirements.txt`. Key ones:

- **ML:** `scikit-learn`, `numpy`, `pandas`
- **App:** `streamlit`
- **Parsing:** `pymupdf` (PyMuPDF), `PyPDF2`, `pdfminer.six`, `python-docx`
- **(Optional) EDA:** `matplotlib`, `seaborn`

Install with:
```bash
pip install -r requirements.txt
```

---

## ğŸ› Troubleshooting

- **Wrong predictions on PDFs**
  - Ensure the app uses a *real* parser (PyMuPDF/PyPDF2/pdfminer). If not, install the parsers from `requirements.txt` and make sure Streamlit runs in the same environment.
  - If the PDF is scanned (image only), OCR it first.

- **â€œFeature-size mismatch: tfidf=â€¦, classifier expects â€¦â€**
  - Youâ€™re loading a mismatched pair of artifacts. Re-train and use the two files generated by the same run (both in `artifacts/`).

- **Windows path errors in the notebook (`unicodeescape`)**
  - Use raw strings or forward slashes:
    ```python
    pdf_path = r"E:\path\to\file.pdf"
    # or
    pdf_path = "E:/path/to/file.pdf"
    ```

---

## ğŸ“ Suggested repo layout

```
.
â”œâ”€â”€ app1.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ resume_classifier_notebook.ipynb
â”œâ”€â”€ data.csv
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ tfidf.pkl
â”‚   â””â”€â”€ clf.pkl
â””â”€â”€ sample_resumes/        # optional test PDFs
    â”œâ”€â”€ Advocate_sample_resume.pdf
    â”œâ”€â”€ Civil_Engineer_sample_resume.pdf
    â”œâ”€â”€ SAP_Developer_sample_resume.pdf
    â””â”€â”€ Network_Security_Engineer_sample_resume.pdf
```

---

## âœ… Notes

- The app returns a **single best category** (no Top-K UI).
- If you prefer a single artifact, you can wrap cleaner+TF-IDF+SVC in a `Pipeline` and save `model.joblib`; the app can then load one file.

---

Happy building! ğŸš€
